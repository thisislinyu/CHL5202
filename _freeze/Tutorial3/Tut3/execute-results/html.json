{
  "hash": "e6e8965e62982609a401d654cb9f9ac7",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Tutorial 3: Logistic Regression 1\"\nauthor: \"Lin Yu\"\ndate: \"2025-02-03\"\nformat: \n  html: \n    toc: true\n    embed-resources: true\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(DT)\nlibrary(here)\n```\n:::\n\n\n\n\n\n\n## Recap of Tutorial 2\n\nLet's review several fundamental statistical concepts: \n\n1. In the following simple linear model, which component(s) are random, and which are fixed/constant?  \n\n\n$Y = X\\beta + \\epsilon \\quad \\text{or} \\quad E(Y\\mid X) = X\\beta, \\quad \\text{where} \\quad \\epsilon \\sim N(0, \\sigma^2)$\n\n:::{.callout-note collapse=\"true\"}\n### Check Answer\n-$Y$ is random and assumed to follow a normal distribution with mean $X\\beta$ and variance$\\sigma^2$ for each individual (recall the LINE assumptions).        \n- The parameter$\\beta$ is a constant (in the frequentist view) but unknown.   \n-$X$ is fixed.  \n-$\\epsilon$ is random.  \n:::\n\n2. What are the estimand, estimator, and estimate?  \n\n:::{.callout-note collapse=\"true\"}\n### Check Answer\nUsing simple linear regression as an example:\n\n- The **estimand** is the parameter we aim to estimate, which is$\\beta$.  \n- An **estimator** is a function of the data used to estimate$\\beta$. The maximum likelihood estimator (MLE) for$\\beta_1$ is:  $\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n} (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^{n} (X_i - \\bar{X})^2}$\n\nDifferent estimators can be proposed; for example, if one proposed $\\tilde{\\beta}_1 = \\bar{Y}$, it would likely fail to capture the true association between $X$ and $Y$.  \n\n- An **estimate** is the specific numerical value obtained from an estimator after applying it to observed data. Since data are random, the estimator $\\hat{\\beta}$ is also random. However, for a specific dataset, the estimate is a fixed number.  \n:::\n\n3. What are standard error and standard deviation? Which varies with sample size and why?  \n\n:::{.callout-note collapse=\"true\"}\n### Check Answer\n- The **standard error (SE)** is the standard deviation of an estimator. It quantifies the variability of the estimator across different samples. Since the precision of an estimator depends on the sample size, a larger sample size leads to a smaller standard error.  \n- The **standard deviation (SD)** measures the variability of the data itself. For example, the variation in heights within a classroom is a fixed property of the data and does not depend on the sample size.  \n:::\n\n4. What is a linear regression model, and which of the following models are linear?  \n\n1.$E(Y\\mid X) = \\beta_1X + \\beta_2 X^2$  \n2.$E(Y\\mid X) = \\beta_1X + \\beta_2 \\log(X)$  \n3.$\\log(E(Y\\mid X)) = \\beta_1X + \\beta_2 X^2$  \n4.$E(Y\\mid X)= \\beta_1X + \\beta_2^2X$  \n\n:::{.callout-note collapse=\"true\"}\n### Check Answer\nAll models except the last one are linear models.  \n\n- **Linearity in regression refers to linearity in parameters, not necessarily in predictors.**  \n- The first three models are linear in regression parameters$\\beta_1$ and$\\beta_2$, even though they include transformations of $X$.  \n- The fourth model is **not** linear because $\\beta_2^2$ introduces a non-linear transformation of a parameter.  \n:::\n\n## Tutorial 3\n\nToday, we'll work on logistic regression.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rms)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: Hmisc\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'Hmisc'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:dplyr':\n\n    src, summarize\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:base':\n\n    format.pval, units\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(here)\nload(here(\"data\",\"tutdata.RData\"))\ndd <- datadist(tutdata)\noptions(datadist = \"dd\")\n\nfit <- lrm(y ~ blood.pressure + sex + age + rcs(cholesterol, 4), data = tutdata)\n\nanova(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                Wald Statistics          Response: y \n\n Factor         Chi-Square d.f. P     \n blood.pressure  0.33      1    0.5640\n sex            13.45      1    0.0002\n age            25.58      1    <.0001\n cholesterol     1.27      3    0.7368\n  Nonlinear      0.78      2    0.6758\n TOTAL          38.63      6    <.0001\n```\n\n\n:::\n\n```{.r .cell-code}\nfit %>% coef()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     Intercept blood.pressure       sex=male            age    cholesterol \n  -0.770428669   -0.002482989    0.477026128    0.033321608   -0.004418957 \n  cholesterol'  cholesterol'' \n   0.022754677   -0.098270674 \n```\n\n\n:::\n\n```{.r .cell-code}\nfit %>% vcov()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   Intercept blood.pressure      sex=male           age\nIntercept       3.0234267326  -2.096123e-03 -1.919335e-04 -1.799902e-03\nblood.pressure -0.0020961233   1.852472e-05 -4.542393e-06 -1.048266e-06\nsex=male       -0.0001919335  -4.542393e-06  1.691277e-02  3.642625e-05\nage            -0.0017999015  -1.048266e-06  3.642625e-05  4.340282e-05\ncholesterol    -0.0155094677  -5.297436e-07 -5.161492e-05 -1.592218e-06\ncholesterol'    0.0379635708   4.065325e-06  1.122060e-04  5.500943e-06\ncholesterol''  -0.1361901376  -1.938549e-05 -3.636389e-04 -2.046384e-05\n                 cholesterol  cholesterol' cholesterol''\nIntercept      -1.550947e-02  3.796357e-02 -1.361901e-01\nblood.pressure -5.297436e-07  4.065325e-06 -1.938549e-05\nsex=male       -5.161492e-05  1.122060e-04 -3.636389e-04\nage            -1.592218e-06  5.500943e-06 -2.046384e-05\ncholesterol     9.119299e-05 -2.315889e-04  8.424284e-04\ncholesterol'   -2.315889e-04  7.331333e-04 -2.936085e-03\ncholesterol''   8.424284e-04 -2.936085e-03  1.241586e-02\n```\n\n\n:::\n\n```{.r .cell-code}\n## point estimate of beta_sex\nbeta_sex_hat <- coef(fit)[\"sex=male\"]\n\n## standard error for coef of sex\nbeta_sex_se <- sqrt(vcov(fit)[3,3])\n\n## we know beta_hat follows a normal distribution, with mean=beta_sex_hat, \n\nalpha <- qnorm(0.975,mean=0,sd=1)\nalpha\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.959964\n```\n\n\n:::\n\n```{.r .cell-code}\nupper <- exp(beta_sex_hat + alpha*beta_sex_se)\nlower <- exp(beta_sex_hat - alpha*beta_sex_se)\n\nlower \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nsex=male \n1.248739 \n```\n\n\n:::\n\n```{.r .cell-code}\nupper \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nsex=male \n2.079064 \n```\n\n\n:::\n:::\n\n\n\n\n\n\nThe odds of the outcome for males is 1.61 times that for females (95% CI of 1.25 to 2.08) controlled for blood pressure, age and cholesterol.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## log odds\nplot(Predict(fit, cholesterol))\n```\n\n::: {.cell-output-display}\n![](Tut3_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\n## probability\nplot(Predict(fit, cholesterol, fun = plogis), ylab = \"Probability\")\n```\n\n::: {.cell-output-display}\n![](Tut3_files/figure-html/unnamed-chunk-3-2.png){width=672}\n:::\n\n```{.r .cell-code}\n## Odds\n\nplot(Predict(fit, cholesterol, fun = exp), ylab = \"Odds\")\n```\n\n::: {.cell-output-display}\n![](Tut3_files/figure-html/unnamed-chunk-3-3.png){width=672}\n:::\n:::\n",
    "supporting": [
      "Tut3_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}