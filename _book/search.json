[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CHL5202 Winter 2025",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1¬† Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. ‚ÄúLiterate Programming.‚Äù Comput. J. 27 (2): 97‚Äì111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "Tutorial1/Tut1.html",
    "href": "Tutorial1/Tut1.html",
    "title": "1¬† Linear Regression",
    "section": "",
    "text": "1.1 Q1\nLoad the data frame tut1 into R from the file tut1.RData available on the course page. The data frame contains the variables x and y.\nCode\nlibrary(here)\nlibrary(texreg)\nlibrary(rms)\nlibrary(kableExtra)\nlibrary(dplyr)\nlibrary(ggplot2)\noptions(prType = \"latex\")\noptions(prType='html')\nload(here(\"data\",'tut1.rdata'))\n\nknitr::opts_chunk$set(warning = FALSE,\n                      message = FALSE)\ntheme_set(\n  theme_bw(base_size = 18) +\n    theme(legend.position = \"bottom\")\n)\nFit a simple linear regression with y as the response and x as the predictor\nfit0 &lt;- ols(y ~ x, data = tut1)\nThe results are identical!\nDiagnostic plots for model checking\npredicted result using restricted cubic spline (skyblue) versus simple linear (black)\nfit_rcs &lt;- ols(y ~ rcs(x, 3), data = tut1)\n\npred_rcs &lt;- tut1 %&gt;% \n  mutate(\n    pred_rcs = predict(fit_rcs,data = tut1)\n  )\n  \npred_rcs %&gt;% ggplot( aes(x =x, y = pred_rcs))+\n # geom_point(size = 0.01)+\n  geom_line(color = \"skyblue\")+\n # geom_point(data =pred_glm, aes(x =x, y = yhat) ,size = 0.01, ) +\ngeom_line(data =pred_glm, aes(x =x, y = yhat),color = \"black\"  ,alpha = 0.7)\nCheck the diagnostic plots again (knots = 3)\nexperiment with different knot values:\nConclusion: knots = 4 is good enough to fit the data! knots = 5 may lead to overfitting.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "Tutorial2/Tut2.html",
    "href": "Tutorial2/Tut2.html",
    "title": "2¬† Model Validation",
    "section": "",
    "text": "2.1 Recap of tutorial 1\nUnderstand random variable through the following simulation:\nn_sample &lt;- 1000\nbeta0 &lt;-  0.5\nbeta1 &lt;- 2\nx &lt;- rnorm(n_sample,mean = 5, sd = 10)\n\nrandom_error &lt;- rnorm(n_sample,mean = 0, sd = 5)\n\ny &lt;- beta0 + beta1*x + random_error\n\npop_dat &lt;- data.frame(y = y,\n           x = x)\n\npop_dat %&gt;% \n  ggplot(aes(x = x, y = y))+\n  geom_point()+\n  theme_bw()\nobserved standard deviation of the data:\npop_dat %&gt;% \n  ggplot(aes(y))+\n  geom_histogram()+\n  theme_bw()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nsd(pop_dat$y)\n\n[1] 20.08596\nsample_size &lt;- 100\nsample_dat &lt;- pop_dat[sample(c(1:sample_size),replace = FALSE),]\n\nsample_dat %&gt;% datatable()\n\n\n\n\nsample_dat %&gt;% \n  ggplot(aes(x = x, y = y))+\n  geom_point()+\n  theme_bw()\nthe standard deviation of the estimator (\\(\\hat\\beta\\)):\nglm_m &lt;- glm(y~x, data= sample_dat)\n\nset.seed(1017)\n## lapply parallel computing: more effecient than a for loop:)\n\ncoef_dat &lt;- lapply(1:100, function(i){\n  sample_dat &lt;- pop_dat[sample(100,replace = FALSE),]\n  glm_m &lt;- glm(y~x, data= sample_dat)\n  coef(glm_m)[2]\n})\n\ncoef_dat %&gt;% head()\n\n[[1]]\n      x \n1.94098 \n\n[[2]]\n      x \n1.94098 \n\n[[3]]\n      x \n1.94098 \n\n[[4]]\n      x \n1.94098 \n\n[[5]]\n      x \n1.94098 \n\n[[6]]\n      x \n1.94098 \n\nset.seed(1017) \nbeta_hat_dat &lt;- lapply(1:100, function(i) {\n  sample_dat &lt;- pop_dat[sample(seq_len(nrow(pop_dat)), ## sample from 1: n_sample \n                               sample_size, ## sample size\n                               replace = FALSE), ]\n  glm_m &lt;- glm(y ~ x, data = sample_dat)\n  coef(glm_m)[2]\n}) %&gt;% unlist()\n\nbeta_hat_dat %&gt;%\n  data.frame() %&gt;% \n  ggplot(aes(.)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\nbeta_hat &lt;- NULL\nfor(sample_size in c(20,100,200,500,1000)){\n  beta_hat_tmp &lt;- lapply(1:100, function(i) {\n  sample_dat &lt;- pop_dat[sample(seq_len(nrow(pop_dat)), ## sample from 1: n_sample \n                               sample_size, ## sample size\n                               replace = FALSE), ]\n  glm_m &lt;- glm(y ~ x, data = sample_dat)\n  coef(glm_m)[2]\n}) %&gt;% unlist() \n  beta_hat &lt;- cbind(beta_hat,beta_hat_tmp )\n}\n\nbeta_hat &lt;- beta_hat %&gt;% data.frame()\n\ncolnames(beta_hat) &lt;- paste0(\"sample size = \",c(20,100,200,500,1000))\n\nbeta_hat %&gt;% \n  datatable()\n\n\n\n\napply(beta_hat,2, sd)\n\n  sample size = 20  sample size = 100  sample size = 200  sample size = 500 \n      1.331442e-01       4.579778e-02       3.317768e-02       1.444121e-02 \nsample size = 1000 \n      2.500723e-15\nviz the distribution of \\(\\hat \\beta\\):\nlibrary(tidyr)\n\nWarning: package 'tidyr' was built under R version 4.3.2\n\nplot_dat &lt;- beta_hat %&gt;% \n  pivot_longer(cols = everything(),\n    values_to = \"beta_hat\",\n    names_to = \"sample size\",\n    names_prefix = \"sample size =\"\n               ) \n  \n plot_dat &lt;- plot_dat %&gt;%  mutate(`sample size` = as.numeric(`sample size`))\n\nplot_dat$`sample size` &lt;- factor(plot_dat$`sample size`, \n                                 levels = c(\"20\",\"100\", \"200\", \"500\", \"1000\"), \n                                 labels = c(\"20\",\"100\", \"200\", \"500\", \"1000\"))\n\n\n  plot_dat %&gt;% \n    ggplot(aes(x = beta_hat,color = `sample size`, fill = `sample size`)) +\n    geom_histogram()+\n    facet_grid(cols = vars(`sample size`),scales = \"free_y\")+\n    theme_bw()+\n    theme(legend.position = \"bottom\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Model Validation</span>"
    ]
  },
  {
    "objectID": "Tutorial2/Tut2.html#tutorial-2",
    "href": "Tutorial2/Tut2.html#tutorial-2",
    "title": "2¬† CHL 5202 Tutorial 2",
    "section": "2.2 Tutorial 2",
    "text": "2.2 Tutorial 2\nUse the hwy2.RData file available from the course page. After attaching the rms package and doing the usual datadist()/options() task\n1. Fit the following model\nfit &lt;- ols(rate~rcs(trks,4)+rcs(sigs1,4)+type+hwy,data=hwy2,x=TRUE,y=TRUE)\n\nrun both the ordinary bootstrap validation and the .632 bootstrap validation on this model. compare the results.\n\n\n\nrm(list = ls())\nlibrary(rms)\nlibrary(dplyr)\n## change your working directory as necessary\nload(\"hwy2.RData\")\n\nstr(hwy2)\n\n'data.frame':   39 obs. of  13 variables:\n $ rate : num  4.58 2.86 3.02 2.29 1.61 6.87 3.85 6.12 3.29 5.88 ...\n $ len  : num  4.99 16.11 9.75 10.65 20.01 ...\n $ ADT  : int  69 73 49 61 28 30 46 25 43 23 ...\n $ trks : int  8 8 10 13 12 6 8 9 12 7 ...\n $ sigs1: num  0.2004 0.0621 0.1026 0.0939 0.05 ...\n $ slim : int  55 60 60 65 70 55 55 55 50 50 ...\n $ shld : int  10 10 10 10 10 10 8 10 4 5 ...\n $ lane : int  8 4 4 6 4 4 4 4 4 4 ...\n $ acpt : num  4.6 4.4 4.7 3.8 2.2 24.8 11 18.5 7.5 8.2 ...\n $ itg  : num  1.2 1.43 1.54 0.94 0.65 0.34 0.47 0.38 0.95 0.12 ...\n $ lwid : int  12 12 12 12 12 12 12 12 12 12 ...\n $ hwy  : Factor w/ 4 levels \"FAI\",\"MA\",\"MC\",..: 1 1 1 1 1 4 4 4 4 4 ...\n $ type : Factor w/ 2 levels \"regular\",\"major\": 2 2 2 2 2 2 2 2 2 2 ...\n\n\n\n\n\n\n\n\nHow do you code factor/nominal variables?\n\n\n\n\n\nWhen working with factor/nominal/categorical variables in R, preprocessing is necessary to ensure the data is properly handled by statistical models and algorithms. Many R functions automatically perform dummy coding when a variable is specified as a factor, typically using the as.factor() function.\nSide note: Dummy coding (sometimes confused with one-hot encoding in CS) is just one way to code factor variables. It is popular because of its ease of interpretation.\n\n\n\n\n\n\n\n\n\nValidate the model\n\n\n\n\n\nA complete process of model validation usually consists of two parts: internal and external validation. Ideally (when you‚Äôre lucky üòÑ), you will have two independent datasets, such as EHR data from two hospitals. You will first build or ( train) and validate your model internally using data from hospital A (the train-validation dataset). Then the model can be sent to hospital B for external validation (test set). If you do not have two datasets, you can manually divide the data into two parts, with one part mimicking the external dataset.\n\nSplit-sample or split-sample averaged(SSA)\nK-fold Cross-validation\nBootstrap: ordinary, .632\n\n\n# sample size 100k\nset.seed(1017)\nn_sample &lt;- 100000\npatient_id &lt;- c(1:n_sample)\n\n# sample with replacement\nsample_dat &lt;- sample(patient_id,size=n_sample,replace = TRUE) \n# proportion of samples selected in bootstrap sampling with replacement\n((sample_dat%&gt;% unique() %&gt;% length())/n_sample )%&gt;% round(3)\n\n[1] 0.632\n\n\n\n\n\n\nh.dd &lt;- datadist(hwy2)\noptions(datadist=\"h.dd\")\nfit &lt;- ols(rate~rcs(trks,4)+\n             rcs(sigs1,4)+type+hwy,\n           data=hwy2,x=TRUE,y=TRUE)\nfit$x %&gt;% head()\n\n  trks      trks'    trks''      sigs1       sigs1'      sigs1'' type=major\n1    8 0.02777778 0.0000000 0.20040080 2.023540e-03 0.0004753511          1\n2    8 0.02777778 0.0000000 0.06207325 7.616807e-07 0.0000000000          1\n3   10 0.75000000 0.2222222 0.10256410 8.221905e-05 0.0000000000          1\n4   13 4.50000000 2.2222222 0.09389671 4.716474e-05 0.0000000000          1\n5   12 3.02777778 1.4074074 0.04997501 0.000000e+00 0.0000000000          1\n6    6 0.00000000 0.0000000 2.00750419 1.016433e+00 0.7779110205          1\n  hwy=MA hwy=MC hwy=PA\n1      0      0      0\n2      0      0      0\n3      0      0      0\n4      0      0      0\n5      0      0      0\n6      0      0      1\n\n\n\nset.seed(1017)\nvalidate(fit, B=100)\n\n\nDivergence or singularity in 14 samples\n\n\n          index.orig training   test optimism index.corrected  n\nR-square      0.7110   0.7542 0.5195   0.2346          0.4764 86\nMSE           1.1106   0.8411 1.8466  -1.0055          2.1161 86\ng             1.8604   1.7997 1.6890   0.1107          1.7497 86\nIntercept     0.0000   0.0000 0.4525  -0.4525          0.4525 86\nSlope         1.0000   1.0000 0.8944   0.1056          0.8944 86\n\n\n\nset.seed(1017)\nvalidate(fit,method = \".632\",B=100)\n\n\nDivergence or singularity in 14 samples\n\n\n          index.orig training    test optimism index.corrected  n\nR-square      0.7110   0.7542 -0.0506   0.4814          0.2297 86\nMSE           1.1106   0.8411  2.8875  -1.1230          2.2336 86\ng             1.8604   1.7997  1.1931   0.4217          1.4386 86\nIntercept     0.0000   0.0000  0.9201  -0.5815          0.5815 86\nSlope         1.0000   1.0000  0.6309   0.2332          0.7668 86\n\n\nBootstrapping with replacement leads to some data points being repeated in the bootstrap sample. As a result, the model may end up ‚Äòmemorizing‚Äô these repeated points, which can cause it to perform better.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>CHL 5202 Tutorial 2</span>"
    ]
  },
  {
    "objectID": "Tutorial3/Tut3.html",
    "href": "Tutorial3/Tut3.html",
    "title": "3¬† Tutorial 3: Logistic Regression 1",
    "section": "",
    "text": "3.1 Recap of Tutorial 2\nLet‚Äôs review several fundamental statistical concepts:\n\\(Y = X\\beta + \\epsilon \\quad \\text{or} \\quad E(Y\\mid X) = X\\beta, \\quad \\text{where} \\quad \\epsilon \\sim N(0, \\sigma^2)\\)\n1.\\(E(Y\\mid X) = \\beta_1X + \\beta_2 X^2\\)\n2.\\(E(Y\\mid X) = \\beta_1X + \\beta_2 \\log(X)\\)\n3.\\(\\log(E(Y\\mid X)) = \\beta_1X + \\beta_2 X^2\\)\n4.\\(E(Y\\mid X)= \\beta_1X + \\beta_2^2X\\)",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Tutorial 3: Logistic Regression 1</span>"
    ]
  },
  {
    "objectID": "Tutorial4/Tut4.html",
    "href": "Tutorial4/Tut4.html",
    "title": "4¬† Logistic Regression 2",
    "section": "",
    "text": "4.1 Recap of Tutorial 3\nlibrary(rms)\n\nLoading required package: Hmisc\n\n\n\nAttaching package: 'Hmisc'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    src, summarize\n\n\nThe following objects are masked from 'package:base':\n\n    format.pval, units\n\nload(here(\"data\",\"tutdata.RData\"))\ndd &lt;- datadist(tutdata)\noptions(datadist = \"dd\")\n\nfit1 &lt;- glm(y ~ blood.pressure + sex + age + cholesterol, data = tutdata,family = binomial(\"logit\"))\n\nsummary(fit1)\n\n\nCall:\nglm(formula = y ~ blood.pressure + sex + age + cholesterol, family = binomial(\"logit\"), \n    data = tutdata)\n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    -1.777923   0.822041  -2.163 0.030555 *  \nblood.pressure -0.002645   0.004299  -0.615 0.538367    \nsexmale         0.474873   0.129886   3.656 0.000256 ***\nage             0.033198   0.006587   5.040 4.66e-07 ***\ncholesterol     0.001845   0.002652   0.696 0.486489    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1379.9  on 999  degrees of freedom\nResidual deviance: 1339.6  on 995  degrees of freedom\nAIC: 1349.6\n\nNumber of Fisher Scoring iterations: 4\n\nanova(fit1)\n\nAnalysis of Deviance Table\n\nModel: binomial, link: logit\n\nResponse: y\n\nTerms added sequentially (first to last)\n\n               Df Deviance Resid. Df Resid. Dev\nNULL                             999     1379.9\nblood.pressure  1   0.2057       998     1379.7\nsex             1  13.0804       997     1366.6\nage             1  26.5172       996     1340.1\ncholesterol     1   0.4846       995     1339.6\n\nfit1 %&gt;% coef()\n\n   (Intercept) blood.pressure        sexmale            age    cholesterol \n  -1.777923213   -0.002645049    0.474872640    0.033197626    0.001845301 \n\nfit1 %&gt;% vcov()\n\n                (Intercept) blood.pressure       sexmale           age\n(Intercept)     0.675751422  -2.230627e-03 -8.100279e-03 -2.126477e-03\nblood.pressure -0.002230627   1.848061e-05 -4.786796e-06 -1.078910e-06\nsexmale        -0.008100279  -4.786796e-06  1.687037e-02  3.554780e-05\nage            -0.002126477  -1.078910e-06  3.554780e-05  4.338693e-05\ncholesterol    -0.001467499   3.526787e-07 -4.926132e-06  3.650156e-07\n                 cholesterol\n(Intercept)    -1.467499e-03\nblood.pressure  3.526787e-07\nsexmale        -4.926132e-06\nage             3.650156e-07\ncholesterol     7.031300e-06\n\n## point estimate of beta_sex\nbeta_sex_hat &lt;- coef(fit1)[\"sexmale\"]\n\n## standard error for coef of sex\nbeta_sex_se &lt;- sqrt(vcov(fit1)[3,3])\n\n## we know beta_hat follows a normal distribution, with mean=beta_sex_hat, \n\nalpha &lt;- qnorm(0.975,mean=0,sd=1)\nalpha\n\n[1] 1.959964\n\nupper &lt;- exp(beta_sex_hat + alpha*beta_sex_se)\nlower &lt;- exp(beta_sex_hat - alpha*beta_sex_se)\n\nlower \n\n sexmale \n1.246452 \n\nupper \n\n sexmale \n2.073928 \n\npredict(fit1,type=\"response\") %&gt;% head()\n\n        1         2         3         4         5         6 \n0.5283084 0.5846051 0.4750915 0.5317948 0.6572165 0.6022435 \n\ndesign_matrix &lt;- model.matrix(y~blood.pressure + sex + age + cholesterol, data=tutdata)\n\ndesign_matrix %&gt;% head()\n\n  (Intercept) blood.pressure sexmale      age cholesterol\n1           1       98.37248       1 39.84991    191.6668\n2           1      112.99815       1 49.20363    168.1024\n3           1      100.90785       0 47.67013    196.4870\n4           1      122.22211       1 41.82732    197.8634\n5           1      125.38972       1 57.72091    200.2004\n6           1      118.71428       1 48.34388    231.3853\n\ndesign_matrix[1,]\n\n   (Intercept) blood.pressure        sexmale            age    cholesterol \n       1.00000       98.37248        1.00000       39.84991      191.66681 \n\nfit1 %&gt;% coef()\n\n   (Intercept) blood.pressure        sexmale            age    cholesterol \n  -1.777923213   -0.002645049    0.474872640    0.033197626    0.001845301 \n\n## linear combination of predictors\nZ &lt;- design_matrix[1,] %*% (fit1 %&gt;% coef())\n\np_hat &lt;- exp(Z)/(1+exp(Z))\n\np_hat\n\n          [,1]\n[1,] 0.5283084",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Tutorial 4: Logistic Regression 2</span>"
    ]
  },
  {
    "objectID": "Tutorial5/Tut5.html",
    "href": "Tutorial5/Tut5.html",
    "title": "5¬† Logistic Regression 3",
    "section": "",
    "text": "5.1 Recap of Tutorial 4\nlibrary(rms)\n\nLoading required package: Hmisc\n\n\n\nAttaching package: 'Hmisc'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    src, summarize\n\n\nThe following objects are masked from 'package:base':\n\n    format.pval, units\n\nload(here(\"data\",\"tutdata.RData\"))\ndd &lt;- datadist(tutdata)\noptions(datadist = \"dd\")\nfit1 &lt;- lrm(y ~ blood.pressure + sex + age + rcs(cholesterol,4), data = tutdata)\nfit2 &lt;- lrm(y ~ blood.pressure + sex * (age + rcs(cholesterol,4)), data = tutdata,x=TRUE, y= TRUE)\nanova(fit2)\n\n                Wald Statistics          Response: y \n\n Factor                                           Chi-Square d.f. P     \n blood.pressure                                    0.26       1   0.6105\n sex  (Factor+Higher Order Factors)               38.71       5   &lt;.0001\n  All Interactions                                26.13       4   &lt;.0001\n age  (Factor+Higher Order Factors)               30.42       2   &lt;.0001\n  All Interactions                                 3.86       1   0.0495\n cholesterol  (Factor+Higher Order Factors)       23.78       6   0.0006\n  All Interactions                                22.47       3   0.0001\n  Nonlinear (Factor+Higher Order Factors)          5.33       4   0.2550\n sex * age  (Factor+Higher Order Factors)          3.86       1   0.0495\n sex * cholesterol  (Factor+Higher Order Factors) 22.47       3   0.0001\n  Nonlinear                                        4.79       2   0.0911\n  Nonlinear Interaction : f(A,B) vs. AB            4.79       2   0.0911\n TOTAL NONLINEAR                                   5.33       4   0.2550\n TOTAL INTERACTION                                26.13       4   &lt;.0001\n TOTAL NONLINEAR + INTERACTION                    26.81       6   0.0002\n TOTAL                                            62.26      10   &lt;.0001\n\nanova(fit2)\n\n                Wald Statistics          Response: y \n\n Factor                                           Chi-Square d.f. P     \n blood.pressure                                    0.26       1   0.6105\n sex  (Factor+Higher Order Factors)               38.71       5   &lt;.0001\n  All Interactions                                26.13       4   &lt;.0001\n age  (Factor+Higher Order Factors)               30.42       2   &lt;.0001\n  All Interactions                                 3.86       1   0.0495\n cholesterol  (Factor+Higher Order Factors)       23.78       6   0.0006\n  All Interactions                                22.47       3   0.0001\n  Nonlinear (Factor+Higher Order Factors)          5.33       4   0.2550\n sex * age  (Factor+Higher Order Factors)          3.86       1   0.0495\n sex * cholesterol  (Factor+Higher Order Factors) 22.47       3   0.0001\n  Nonlinear                                        4.79       2   0.0911\n  Nonlinear Interaction : f(A,B) vs. AB            4.79       2   0.0911\n TOTAL NONLINEAR                                   5.33       4   0.2550\n TOTAL INTERACTION                                26.13       4   &lt;.0001\n TOTAL NONLINEAR + INTERACTION                    26.81       6   0.0002\n TOTAL                                            62.26      10   &lt;.0001\ncompare two models using LRT. To use LRT, one model needs to be nested within a larger model.\nlrtest(fit1,fit2)\n\n\nModel 1: y ~ blood.pressure + sex + age + rcs(cholesterol, 4)\nModel 2: y ~ blood.pressure + sex * (age + rcs(cholesterol, 4))\n\n  L.R. Chisq         d.f.            P \n2.831616e+01 4.000000e+00 1.076137e-05",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Tutorial 5: Logistic Regression 3</span>"
    ]
  },
  {
    "objectID": "Tutorial5/Tut5.html#boostrapping",
    "href": "Tutorial5/Tut5.html#boostrapping",
    "title": "5¬† Logistic Regression 3",
    "section": "5.3 Boostrapping",
    "text": "5.3 Boostrapping\nresampling technique used to estimate the sampling distribution of a statistic by repeatedly sampling with replacement from the original dataset, useful when theoretical distributions are unknown or difficult to derive.\nfor example, say we want to calculate the confidence interval for sample mean, we need to know the distribution of the sample mean:\n\nset.seed(1017)\n\n## pop data\nn &lt;- 1000\ndata &lt;- rnorm(n, mean=5, sd=10) \n\nanalytical_lower &lt;- mean(data) - 1.96*sd(data)/sqrt(n)\nanalytical_upper &lt;- mean(data) + 1.96*sd(data)/sqrt(n)\n\n\nnboot &lt;- 1000\n\n## sample(data, size=n, replace=TRUE) bootstrap sampling\n## mean(sample(data, size=n, replace=TRUE)) calculate sample mean\nboot_means &lt;- replicate(nboot, mean(sample(data, size=n, replace=TRUE)))\n\nboot_means %&gt;% density() %&gt;% plot()\n\n\n\n\n\n\n\n# percentile \nboot_lower_bound &lt;- quantile(boot_means, 0.025)\nboot_upper_bound &lt;- quantile(boot_means, 0.975)\n\n\ncat(\"Original Sample Mean:\", mean(data), \"\\n\")\n\nOriginal Sample Mean: 4.87006 \n\ncat(\"95% CI for Mean (analytical solution):\", analytical_lower, analytical_upper, \"\\n\")\n\n95% CI for Mean (analytical solution): 4.247337 5.492783 \n\ncat(\"95% Bootstrapped (nboot=1000) CI for Mean:\", boot_lower_bound, boot_upper_bound, \"\\n\")\n\n95% Bootstrapped (nboot=1000) CI for Mean: 4.220525 5.464462 \n\n\nwhat would happen if we increase the number of bootstrap?\n\nnboot &lt;- 10000\n\n## sample(data, size=n, replace=TRUE) bootstrap sampling\n## mean(sample(data, size=n, replace=TRUE)) calculate sample mean\nboot_means2 &lt;- replicate(nboot, mean(sample(data, size=n, replace=TRUE)))\n\n# percentile \nboot_lower_bound &lt;- quantile(boot_means2, 0.025)\nboot_upper_bound &lt;- quantile(boot_means2, 0.975)\ncat(\"95% CI for Mean (analytical solution):\", analytical_lower, analytical_upper, \"\\n\")\n\n95% CI for Mean (analytical solution): 4.247337 5.492783 \n\ncat(\"95% Bootstrapped(boot=10000) CI for Mean:\", boot_lower_bound, boot_upper_bound, \"\\n\")\n\n95% Bootstrapped(boot=10000) CI for Mean: 4.242179 5.499558",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Tutorial 5: Logistic Regression 3</span>"
    ]
  },
  {
    "objectID": "Tutorial6/Tut6.html",
    "href": "Tutorial6/Tut6.html",
    "title": "6¬† Survival Analysis 1",
    "section": "",
    "text": "6.1 Recap of Tutorial 5\nOne question left from last week:\nSuppose you fit a generalized linear model with X as covariate. You are asked to calculate the effect size of X on the outcome Y comparing X=3 to X=2, and the confidence interval of the effect size:\n\\[\ng[E(Y\\mid X) ] = \\beta_1 + \\beta_2 \\mathbf{1}{\\{X=2\\}}+\\beta_3 \\mathbf{1}{\\{X=3\\}}\n\\]\n## More about confidence interval \nset.seed(1017)\nx &lt;- sample(1:3, size=1000, replace=TRUE, prob=c(0.2, 0.2, 0.6))\nhead(x)\n\n[1] 3 3 2 1 1 2\n\ndat_x &lt;- data.frame(x = x %&gt;% as.factor())\n\ndesign_matrix &lt;- model.matrix(~., data = dat_x)\n\nhead(design_matrix)\n\n  (Intercept) x2 x3\n1           1  0  1\n2           1  0  1\n3           1  1  0\n4           1  0  0\n5           1  0  0\n6           1  1  0\n\ntrue_coef &lt;- c(0.5,1,2)\ntrue_Y_linear &lt;-   design_matrix %*% true_coef\n\n random_error &lt;- rnorm(  length(true_Y_linear), mean=0,sd=1) \n  \ntrue_Y_cont &lt;- true_Y_linear + random_error\n  \n   \n  true_Y_prob &lt;- exp(true_Y_linear)/(1+exp(true_Y_linear)) ## location param for logistic model\n  true_Y_bin &lt;- apply(true_Y_prob, 1, function(x) rbinom(n =1, size =1, prob = x)) ## for logistic model, sampling Y\n  \n  \nsim_dat &lt;-   data.frame(x = as.factor(x),\n             true_Y_cont = true_Y_cont,\n             true_Y_bin = true_Y_bin\n             )\n\noutcome_m &lt;- glm(true_Y_bin ~ x, family = binomial(\"logit\"), data = sim_dat)\nsummary(outcome_m)\n\n\nCall:\nglm(formula = true_Y_bin ~ x, family = binomial(\"logit\"), data = sim_dat)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   0.6792     0.1447   4.694 2.67e-06 ***\nx2            1.0489     0.2541   4.128 3.65e-05 ***\nx3            1.7758     0.2087   8.509  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 834.94  on 999  degrees of freedom\nResidual deviance: 760.88  on 997  degrees of freedom\nAIC: 766.88\n\nNumber of Fisher Scoring iterations: 5",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Survival Analysis 1</span>"
    ]
  },
  {
    "objectID": "Tutorial6/Tut6.html#method-1",
    "href": "Tutorial6/Tut6.html#method-1",
    "title": "6¬† CHL 5202 Tutorial 6",
    "section": "6.1 Method 1",
    "text": "6.1 Method 1\nChange the reference level:\n\nsim_dat$x &lt;- relevel(sim_dat$x, ref = \"2\")\n\noutcome_m2 &lt;- glm(true_Y_bin ~ x, family = binomial(\"logit\"), data = sim_dat)\n\nsummary(outcome_m2)\n\n\nCall:\nglm(formula = true_Y_bin ~ x, family = binomial(\"logit\"), data = sim_dat)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   1.7280     0.2088   8.274  &lt; 2e-16 ***\nx1           -1.0489     0.2541  -4.128 3.65e-05 ***\nx3            0.7269     0.2574   2.824  0.00474 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 834.94  on 999  degrees of freedom\nResidual deviance: 760.88  on 997  degrees of freedom\nAIC: 766.88\n\nNumber of Fisher Scoring iterations: 5",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>CHL 5202 Tutorial 6</span>"
    ]
  },
  {
    "objectID": "Tutorial6/Tut6.html#method-2",
    "href": "Tutorial6/Tut6.html#method-2",
    "title": "6¬† CHL 5202 Tutorial 6",
    "section": "6.2 Method 2",
    "text": "6.2 Method 2\nAsk yourself the following questions:\n\nwhat is the estimand/goal?\n\n\\[\\beta_3 - \\beta_2\\]\n\nwhat is the estimator?\n\n\\[\\hat\\beta_3 - \\hat \\beta_2\\]\n\nwhat is the distribution of MLE estimator?\n\n\\(\\hat\\beta_3\\) and \\(\\hat \\beta_2\\) follow normal distribution. (one of the nice properties of MLE estimators)\nthen we can easily calculate the variance of \\[\\hat\\beta_3 - \\hat \\beta_2\\] using some math.\n\ncov_matrix &lt;- vcov(outcome_m)\n\nvar2 &lt;- cov_matrix[2,2]\nvar3 &lt;- cov_matrix[3,3]\ncov_b23 &lt;- cov_matrix[2,3]\n## refer to tutorial 4 for the calculation of var(A+B)\nvar_b3_minus_b2 &lt;- var2 + var3  - 2*cov_b23\n\nvar_b3_minus_b2 %&gt;% sqrt()\n\n[1] 0.2573678\n\nse &lt;- var_b3_minus_b2 %&gt;% sqrt()\n\nse\n\n[1] 0.2573678\n\n\nthe point estiamte is calculated as:\n\npoint_est &lt;- coef(outcome_m)[3]-coef(outcome_m)[2]\n\nso we conclude that \\(\\hat\\beta_3 - \\hat \\beta_2\\) follow a normal distribution with mean 0.7269048 and se 0.2573678\nFinaly question, what is the relationship between \\(\\hat\\beta_3 - \\hat \\beta_2\\) and OR??\n\\[log(OR) = \\hat\\beta_3 - \\hat \\beta_2\\] You can calculate the point estimate and confidence interval for the OR as well! Hooray!\n\n6.2.1 Tutorial 6\nIn this and the following few tutorials, we will focus on survival analysis.\n\nlibrary(rms)\n\nLoading required package: Hmisc\n\n\n\nAttaching package: 'Hmisc'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    src, summarize\n\n\nThe following objects are masked from 'package:base':\n\n    format.pval, units\n\nlibrary(survival)\n\nWarning: package 'survival' was built under R version 4.3.2\n\nstr(mgus2)\n\n'data.frame':   1384 obs. of  11 variables:\n $ id    : num  1 2 3 4 5 6 7 8 9 10 ...\n $ age   : num  88 78 94 68 90 90 89 87 86 79 ...\n  ..- attr(*, \"label\")= chr \"AGE AT mgus_sp\"\n $ sex   : Factor w/ 2 levels \"F\",\"M\": 1 1 2 2 1 2 1 1 1 1 ...\n  ..- attr(*, \"label\")= chr \"Sex\"\n  ..- attr(*, \"format\")= chr \"$desex\"\n $ dxyr  : num  1981 1968 1980 1977 1973 ...\n $ hgb   : num  13.1 11.5 10.5 15.2 10.7 12.9 10.5 12.3 14.5 9.4 ...\n $ creat : num  1.3 1.2 1.5 1.2 0.8 1 0.9 1.2 0.9 1.1 ...\n $ mspike: num  0.5 2 2.6 1.2 1 0.5 1.3 1.6 2.4 2.3 ...\n  ..- attr(*, \"label\")= chr \"SEP SPIKE AT mgus_sp\"\n $ ptime : num  30 25 46 92 8 4 151 2 57 136 ...\n $ pstat : num  0 0 0 0 0 0 0 0 0 0 ...\n $ futime: num  30 25 46 92 8 4 151 2 57 136 ...\n $ death : num  1 1 1 1 1 1 1 1 0 1 ...\n\n\n\nfit1 &lt;- npsurv(Surv(futime,death)~sex,data=mgus2)\nlibrary(dplyr)\ntmp1 &lt;- summary(fit1) %&gt;% head(5) %&gt;% unlist()\ntmp &lt;- summary(fit1)\n\nkm_dat &lt;- data.frame(time = tmp$time,\n                       risk = tmp$n.risk,\n                     death = tmp$n.event) %&gt;%\n  mutate(surv_i = 1- death/risk)\n\n\n\nsurvplot(fit1,n.risk=TRUE)\n\n\n\n\n\n\n\n\n\nprint(fit1.mh &lt;- survdiff(Surv(futime, death) ~ sex, data = mgus2))\n\nCall:\nsurvdiff(formula = Surv(futime, death) ~ sex, data = mgus2)\n\n        N Observed Expected (O-E)^2/E (O-E)^2/V\nsex=F 631      423      471      4.88      9.67\nsex=M 753      540      492      4.67      9.67\n\n Chisq= 9.7  on 1 degrees of freedom, p= 0.002 \n\n\n\nsummary(fit1,times=12) # time is in months\n\nCall: npsurv(formula = Surv(futime, death) ~ sex, data = mgus2)\n\n                sex=F \n        time       n.risk      n.event     survival      std.err lower 95% CI \n     12.0000     569.0000      61.0000       0.9032       0.0118       0.8804 \nupper 95% CI \n      0.9266 \n\n                sex=M \n        time       n.risk      n.event     survival      std.err lower 95% CI \n      12.000      646.000      112.000        0.851        0.013        0.826 \nupper 95% CI \n       0.877 \n\n\n\n## risk difference at year 1\nRD &lt;- 0.9032- 0.851\nRD + 1.96*sqrt(0.0118^2 +0.013^2)\n\n[1] 0.08661126\n\nRD - 1.96*sqrt(0.0118^2 +0.013^2)\n\n[1] 0.01778874\n\n\nwe know that logHR follows a normal distribution. So the confidence interval for HR could be calculated as:\n\\[exp(logHR ¬± 1.96 \\cdot se_{logHR}) = exp(logHR)\\cdot exp(¬± 1.96 \\cdot se_{logHR}) = HR \\cdot exp(¬± 1.96 \\cdot se_{logHR}) \\]\n\n## HR\n## HR &lt;- (obs1/exp1)/(obs2/exp2)\nHR &lt;- (423/471)/(540/492)\n\nHR\n\n[1] 0.818259\n\n## variance_logHR &lt;- (1/exp1) + (1/exp2)\nse_logHR &lt;- sqrt( (1/471) +(1/492) )\n\n\n## lower bound\nHR *exp(-1.96*se_logHR)\n\n[1] 0.7211367\n\n## 95% CI\n## upper bound\nHR *exp(1.96*se_logHR)\n\n[1] 0.9284618",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>CHL 5202 Tutorial 6</span>"
    ]
  },
  {
    "objectID": "Tutorial7/Tut7.html",
    "href": "Tutorial7/Tut7.html",
    "title": "7¬† Survival Analysis 2",
    "section": "",
    "text": "7.1 Recap of Tutorial 6\nWe went over how KM estimator (for survival probability) is constructed for discrete time case. The idea being that we look for the connections among survial function, hazard function and the cumulative density function.\nThe derivations can be found here\nThe non-parametric estimator for hazard is quite intuitive.\n\\[\\hat \\lambda(t_j) = \\frac{d_j}{r_j}\\]\nwhere \\(d_j\\) is the number of death at time \\(j\\), and \\(r_j\\) is the number of participants at risk at time \\(j\\)\nThe KM/product-limit estimator (for survival): \\[\\hat S(t_j) = \\prod_{i=1}^{j}(1- \\hat \\lambda(t_i))\\]\nan example can be found here\nhere is my cheatsheet for survival analysis basics.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Survival Analysis 2</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. ‚ÄúLiterate Programming.‚Äù Comput.\nJ. 27 (2): 97‚Äì111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "Tutorial1/Tut1.html#q1",
    "href": "Tutorial1/Tut1.html#q1",
    "title": "1¬† Linear Regression",
    "section": "",
    "text": "Note\n\n\n\nor you can use glm (generalized linear model) or lm (linear model)\n\nfit_glm &lt;- glm(y~x, data= tut1)\nfit_lm &lt;- lm(y~x, data = tut1)\n\n\n\n\n\nCode\nhtmlreg(list(fit0, fit_lm,fit_glm), \n        ci.force = TRUE,\n        #custom.coef.map = keepvars,\n        custom.model.names = c(\"ols\", \"lm\",\"glm\"))\n\n\n\nStatistical models\n\n\n\n\n¬†\n\n\nols\n\n\nlm\n\n\nglm\n\n\n\n\n\n\nIntercept\n\n\n1.22*\n\n\n¬†\n\n\n¬†\n\n\n\n\n¬†\n\n\n[ 0.80; 1.64]\n\n\n¬†\n\n\n¬†\n\n\n\n\nx\n\n\n-0.36*\n\n\n-0.36*\n\n\n-0.36*\n\n\n\n\n¬†\n\n\n[-0.48; -0.24]\n\n\n[-0.48; -0.24]\n\n\n[-0.48; -0.24]\n\n\n\n\n(Intercept)\n\n\n¬†\n\n\n1.22*\n\n\n1.22*\n\n\n\n\n¬†\n\n\n¬†\n\n\n[ 0.80; 1.64]\n\n\n[ 0.80; 1.64]\n\n\n\n\nNum. obs.\n\n\n100\n\n\n100\n\n\n100\n\n\n\n\nR2\n\n\n0.27\n\n\n0.27\n\n\n¬†\n\n\n\n\nAdj. R2\n\n\n0.27\n\n\n0.27\n\n\n¬†\n\n\n\n\nL.R.\n\n\n32.07\n\n\n¬†\n\n\n¬†\n\n\n\n\nAIC\n\n\n¬†\n\n\n¬†\n\n\n303.66\n\n\n\n\nBIC\n\n\n¬†\n\n\n¬†\n\n\n311.48\n\n\n\n\nLog Likelihood\n\n\n¬†\n\n\n¬†\n\n\n-148.83\n\n\n\n\nDeviance\n\n\n¬†\n\n\n¬†\n\n\n114.89\n\n\n\n\n\n\n* Null hypothesis value outside the confidence interval.\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf the model is correctly specified, and all assumptions (known as LINE) are met, the residuals should be randomly scattered around zero\n\n\npred_glm &lt;- tut1 %&gt;% \n             mutate(\n               yobs = y,\n           yhat = predict(fit_glm),\n           resid = y - yhat)\n\nx_y_plot &lt;- ggplot(pred_glm, aes(x,yobs) )+\n  geom_point()+\n  geom_smooth()\n\nx_resid_plot &lt;- ggplot(pred_glm, aes(x,resid) )+\n  geom_point()+\n  geom_smooth()\n\n\nyhat_resid_plot &lt;- ggplot(pred_glm, aes(yhat,resid) )+\n  geom_point()+\n  geom_smooth()\n\nx_y_plot\nx_resid_plot \nyhat_resid_plot\n\n\n\n\npred_rcs &lt;- tut1 %&gt;% \n             mutate(\n               yobs = y,\n           yhat = predict(fit_rcs),\n           resid = y - yhat)\n\nx_y_plot &lt;- ggplot(pred_rcs, aes(x,yobs) )+\n  geom_point()+\n  geom_smooth()\n\nx_resid_plot &lt;- ggplot(pred_rcs, aes(x,resid) )+\n  geom_point()+\n  geom_smooth()\n\n\nyhat_resid_plot &lt;- ggplot(pred_rcs, aes(yhat,resid) )+\n  geom_point()+\n  geom_smooth()\n\nx_y_plot\nx_resid_plot \nyhat_resid_plot\n\n\n\nknot = 4knot = 5\n\n\n\nCode\nfit_rcs &lt;- ols(y ~ rcs(x, 5), data = tut1)\npred_rcs &lt;- tut1 %&gt;% \n             mutate(\n               yobs = y,\n           yhat = predict(fit_rcs),\n           resid = y - yhat)\n\nx_y_plot &lt;- ggplot(pred_rcs, aes(x,yobs) )+\n  geom_point()+\n  geom_smooth()\n\nx_resid_plot &lt;- ggplot(pred_rcs, aes(x,resid) )+\n  geom_point()+\n  geom_smooth()\n\n\nyhat_resid_plot &lt;- ggplot(pred_rcs, aes(yhat,resid) )+\n  geom_point()+\n  geom_smooth()\n\nx_y_plot\nx_resid_plot \nyhat_resid_plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nfit_rcs &lt;- ols(y ~ rcs(x, 5), data = tut1)\npred_rcs &lt;- tut1 %&gt;% \n             mutate(\n               yobs = y,\n           yhat = predict(fit_rcs),\n           resid = y - yhat)\n\nx_y_plot &lt;- ggplot(pred_rcs, aes(x,yobs) )+\n  geom_point()+\n  geom_smooth()\n\nx_resid_plot &lt;- ggplot(pred_rcs, aes(x,resid) )+\n  geom_point()+\n  geom_smooth()\n\n\nyhat_resid_plot &lt;- ggplot(pred_rcs, aes(yhat,resid) )+\n  geom_point()+\n  geom_smooth()\n\nx_y_plot\nx_resid_plot \nyhat_resid_plot",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "Tutorial1/Tut1.html#q2",
    "href": "Tutorial1/Tut1.html#q2",
    "title": "1¬† Linear Regression",
    "section": "1.2 Q2",
    "text": "1.2 Q2\n\nQ2 is a similar question but with more than one predictor!\n\nLoad the data frame FEV from the file FEV.RData. For these data, use the variable fev as the response and the rest as the explanatory covariates.\n\nload(here('data','FEV.rdata'))\n\n\nFEV %&gt;% \n  head() %&gt;% \n  kable()\n\n\n\n\nid\nage\nfev\nheight\nsex\nsmoke\n\n\n\n\n301\n9\n1.708\n57.0\nfemale\nnon-current smoker\n\n\n451\n8\n1.724\n67.5\nfemale\nnon-current smoker\n\n\n501\n7\n1.720\n54.5\nfemale\nnon-current smoker\n\n\n642\n9\n1.558\n53.0\nmale\nnon-current smoker\n\n\n901\n9\n1.895\n57.0\nmale\nnon-current smoker\n\n\n1701\n8\n2.336\n61.0\nfemale\nnon-current smoker\n\n\n\n\n\n\n\nFit an additive linear model to fev using the other variables as the covariates. Evaluate whether any of the continuous variables should be fit as non-linear terms.\nfit the linear model\nfev_lm &lt;- ols(fev ~ age + height + sex + smoke,\n               data = FEV)\n\npred_fev &lt;- FEV %&gt;% \n             mutate(\n               yobs = fev,\n           yhat = predict(fev_lm),\n           resid = yobs - yhat)\n\nage_resid_plot &lt;- ggplot(pred_fev, aes(age,resid) )+\n  geom_point()+\n  geom_smooth()\n\nheight_resid_plot &lt;- ggplot(pred_fev, aes(height,resid) )+\n  geom_point()+\n  geom_smooth()\n\nage_resid_plot\nheight_resid_plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat do the residual plots indicate?\n\n\n\nnon-linear relationship!\n\n\nFit RCS:\n\nfev_rcs &lt;- ols(fev ~ rcs(age, 4) + rcs(height, 4) + sex + smoke,\n                data = FEV)\n\nAgain, check the diagnostic plots:\npred_fev &lt;- FEV %&gt;% \n             mutate(\n               yobs = fev,\n           yhat = predict(fev_rcs),\n           resid = yobs - yhat)\n\nage_y_plot &lt;- ggplot(pred_fev, aes(age,yobs) )+\n  geom_point()+\n  geom_smooth()\n\nage_resid_plot &lt;- ggplot(pred_fev, aes(age,resid) )+\n  geom_point()+\n  geom_smooth()\n\n\nyhat_resid_plot &lt;- ggplot(pred_fev, aes(yhat,resid) )+\n  geom_point()+\n  geom_smooth()\n\nage_y_plot\nage_resid_plot \nyhat_resid_plot\n\n\n\n\n\n\n\n\n\n\n\n\n\nheight_y_plot &lt;- ggplot(pred_fev, aes(height,yobs) )+\n  geom_point()+\n  geom_smooth()\n\nheight_resid_plot &lt;- ggplot(pred_fev, aes(height,resid) )+\n  geom_point()+\n  geom_smooth()\n\n\nyhat_resid_plot &lt;- ggplot(pred_fev, aes(yhat,resid) )+\n  geom_point()+\n  geom_smooth()\n\nheight_y_plot\nheight_resid_plot \nyhat_resid_plot",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "Tutorial2/Tut2.html#tutorial-2-model-validation",
    "href": "Tutorial2/Tut2.html#tutorial-2-model-validation",
    "title": "2¬† Model Validation",
    "section": "2.2 Tutorial 2: Model Validation",
    "text": "2.2 Tutorial 2: Model Validation\nUse the hwy2.RData file available from the course page. After attaching the rms package and doing the usual datadist()/options() task\n1. Fit the following model\nfit &lt;- ols(rate~rcs(trks,4)+rcs(sigs1,4)+type+hwy,data=hwy2,x=TRUE,y=TRUE)\n\nrun both the ordinary bootstrap validation and the .632 bootstrap validation on this model. compare the results.\n\n\n\nrm(list = ls())\nlibrary(rms)\nlibrary(dplyr)\n## change your working directory as necessary\nload(here(\"data\",\"hwy2.RData\"))\n\nstr(hwy2)\n\n'data.frame':   39 obs. of  13 variables:\n $ rate : num  4.58 2.86 3.02 2.29 1.61 6.87 3.85 6.12 3.29 5.88 ...\n $ len  : num  4.99 16.11 9.75 10.65 20.01 ...\n $ ADT  : int  69 73 49 61 28 30 46 25 43 23 ...\n $ trks : int  8 8 10 13 12 6 8 9 12 7 ...\n $ sigs1: num  0.2004 0.0621 0.1026 0.0939 0.05 ...\n $ slim : int  55 60 60 65 70 55 55 55 50 50 ...\n $ shld : int  10 10 10 10 10 10 8 10 4 5 ...\n $ lane : int  8 4 4 6 4 4 4 4 4 4 ...\n $ acpt : num  4.6 4.4 4.7 3.8 2.2 24.8 11 18.5 7.5 8.2 ...\n $ itg  : num  1.2 1.43 1.54 0.94 0.65 0.34 0.47 0.38 0.95 0.12 ...\n $ lwid : int  12 12 12 12 12 12 12 12 12 12 ...\n $ hwy  : Factor w/ 4 levels \"FAI\",\"MA\",\"MC\",..: 1 1 1 1 1 4 4 4 4 4 ...\n $ type : Factor w/ 2 levels \"regular\",\"major\": 2 2 2 2 2 2 2 2 2 2 ...\n\n\n\n\n\n\n\n\nHow do you code factor/nominal variables?\n\n\n\n\n\nWhen working with factor/nominal/categorical variables in R, preprocessing is necessary to ensure the data is properly handled by statistical models and algorithms. Many R functions automatically perform dummy coding when a variable is specified as a factor, typically using the as.factor() function.\nSide note: Dummy coding (sometimes confused with one-hot encoding in CS) is just one way to code factor variables. It is popular because of its ease of interpretation.\n\n\n\n\n\n\n\n\n\nValidate the model\n\n\n\n\n\nA complete process of model validation usually consists of two parts: internal and external validation. Ideally (when you‚Äôre lucky üòÑ), you will have two independent datasets, such as EHR data from two hospitals. You will first build or ( train) and validate your model internally using data from hospital A (the train-validation dataset). Then the model can be sent to hospital B for external validation (test set). If you do not have two datasets, you can manually divide the data into two parts, with one part mimicking the external dataset.\n\nSplit-sample or split-sample averaged(SSA)\nK-fold Cross-validation\nBootstrap: ordinary, .632\n\n\n# sample size 100k\nset.seed(1017)\nn_sample &lt;- 100000\npatient_id &lt;- c(1:n_sample)\n\n# sample with replacement\nsample_dat &lt;- sample(patient_id,size=n_sample,replace = TRUE) \n# proportion of samples selected in bootstrap sampling with replacement\n((sample_dat%&gt;% unique() %&gt;% length())/n_sample )%&gt;% round(3)\n\n[1] 0.632\n\n\n\n\n\n\nh.dd &lt;- datadist(hwy2)\noptions(datadist=\"h.dd\")\nfit &lt;- ols(rate~rcs(trks,4)+\n             rcs(sigs1,4)+type+hwy,\n           data=hwy2,x=TRUE,y=TRUE)\nfit$x %&gt;% head()\n\n  trks      trks'    trks''      sigs1       sigs1'      sigs1'' type=major\n1    8 0.02777778 0.0000000 0.20040080 2.023540e-03 0.0004753511          1\n2    8 0.02777778 0.0000000 0.06207325 7.616807e-07 0.0000000000          1\n3   10 0.75000000 0.2222222 0.10256410 8.221905e-05 0.0000000000          1\n4   13 4.50000000 2.2222222 0.09389671 4.716474e-05 0.0000000000          1\n5   12 3.02777778 1.4074074 0.04997501 0.000000e+00 0.0000000000          1\n6    6 0.00000000 0.0000000 2.00750419 1.016433e+00 0.7779110205          1\n  hwy=MA hwy=MC hwy=PA\n1      0      0      0\n2      0      0      0\n3      0      0      0\n4      0      0      0\n5      0      0      0\n6      0      0      1\n\n\n\nset.seed(1017)\nvalidate(fit, B=100)\n\n\nDivergence or singularity in 14 samples\n\n\n          index.orig training   test optimism index.corrected  n\nR-square      0.7110   0.7542 0.5195   0.2346          0.4764 86\nMSE           1.1106   0.8411 1.8466  -1.0055          2.1161 86\ng             1.8604   1.7997 1.6890   0.1107          1.7497 86\nIntercept     0.0000   0.0000 0.4525  -0.4525          0.4525 86\nSlope         1.0000   1.0000 0.8944   0.1056          0.8944 86\n\n\n\nset.seed(1017)\nvalidate(fit,method = \".632\",B=100)\n\n\nDivergence or singularity in 14 samples\n\n\n          index.orig training    test optimism index.corrected  n\nR-square      0.7110   0.7542 -0.0506   0.4814          0.2297 86\nMSE           1.1106   0.8411  2.8875  -1.1230          2.2336 86\ng             1.8604   1.7997  1.1931   0.4217          1.4386 86\nIntercept     0.0000   0.0000  0.9201  -0.5815          0.5815 86\nSlope         1.0000   1.0000  0.6309   0.2332          0.7668 86\n\n\nBootstrapping with replacement leads to some data points being repeated in the bootstrap sample. As a result, the model may end up ‚Äòmemorizing‚Äô these repeated points, which can cause it to perform better.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Model Validation</span>"
    ]
  },
  {
    "objectID": "Tutorial3/Tut3.html#recap-of-tutorial-2",
    "href": "Tutorial3/Tut3.html#recap-of-tutorial-2",
    "title": "3¬† Tutorial 3: Logistic Regression 1",
    "section": "",
    "text": "In the following simple linear model, which component(s) are random, and which are fixed/constant?\n\n\n\n\n\n\n\n\nCheck Answer\n\n\n\n\n\n-\\(Y\\) is random and assumed to follow a normal distribution with mean \\(X\\beta\\) and variance\\(\\sigma^2\\) for each individual (recall the LINE assumptions).\n- The parameter\\(\\beta\\) is a constant (in the frequentist view) but unknown.\n-\\(X\\) is fixed.\n-\\(\\epsilon\\) is random.\n\n\n\n\nWhat are the estimand, estimator, and estimate?\n\n\n\n\n\n\n\nCheck Answer\n\n\n\n\n\nUsing simple linear regression as an example:\n\nThe estimand is the parameter we aim to estimate, which is\\(\\beta\\).\n\nAn estimator is a function of the data used to estimate\\(\\beta\\). The maximum likelihood estimator (MLE) for\\(\\beta_1\\) is: \\(\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n} (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^{n} (X_i - \\bar{X})^2}\\)\n\nDifferent estimators can be proposed; for example, if one proposed \\(\\tilde{\\beta}_1 = \\bar{Y}\\), it would likely fail to capture the true association between \\(X\\) and \\(Y\\).\n\nAn estimate is the specific numerical value obtained from an estimator after applying it to observed data. Since data are random, the estimator \\(\\hat{\\beta}\\) is also random. However, for a specific dataset, the estimate is a fixed number.\n\n\n\n\n\n\nWhat are standard error and standard deviation? Which varies with sample size and why?\n\n\n\n\n\n\n\nCheck Answer\n\n\n\n\n\n\nThe standard error (SE) is the standard deviation of an estimator. It quantifies the variability of the estimator across different samples. Since the precision of an estimator depends on the sample size, a larger sample size leads to a smaller standard error.\n\nThe standard deviation (SD) measures the variability of the data itself. For example, the variation in heights within a classroom is a fixed property of the data and does not depend on the sample size.\n\n\n\n\n\n\nWhat is a linear regression model, and which of the following models are linear?\n\n\n\n\n\n\n\n\nCheck Answer\n\n\n\n\n\nAll models except the last one are linear models.\n\nLinearity in regression refers to linearity in parameters, not necessarily in predictors.\n\nThe first three models are linear in regression parameters\\(\\beta_1\\) and\\(\\beta_2\\), even though they include transformations of \\(X\\).\n\nThe fourth model is not linear because \\(\\beta_2^2\\) introduces a non-linear transformation of a parameter.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Tutorial 3: Logistic Regression 1</span>"
    ]
  },
  {
    "objectID": "Tutorial3/Tut3.html#tutorial-3",
    "href": "Tutorial3/Tut3.html#tutorial-3",
    "title": "3¬† Tutorial 3: Logistic Regression 1",
    "section": "3.2 Tutorial 3",
    "text": "3.2 Tutorial 3\nToday, we‚Äôll work on logistic regression.\n\nlibrary(rms)\n\nLoading required package: Hmisc\n\n\n\nAttaching package: 'Hmisc'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    src, summarize\n\n\nThe following objects are masked from 'package:base':\n\n    format.pval, units\n\nlibrary(here)\nload(here(\"data\",\"tutdata.RData\"))\ndd &lt;- datadist(tutdata)\noptions(datadist = \"dd\")\n\nfit &lt;- lrm(y ~ blood.pressure + sex + age + rcs(cholesterol, 4), data = tutdata)\n\nanova(fit)\n\n                Wald Statistics          Response: y \n\n Factor         Chi-Square d.f. P     \n blood.pressure  0.33      1    0.5640\n sex            13.45      1    0.0002\n age            25.58      1    &lt;.0001\n cholesterol     1.27      3    0.7368\n  Nonlinear      0.78      2    0.6758\n TOTAL          38.63      6    &lt;.0001\n\nfit %&gt;% coef()\n\n     Intercept blood.pressure       sex=male            age    cholesterol \n  -0.770428669   -0.002482989    0.477026128    0.033321608   -0.004418957 \n  cholesterol'  cholesterol'' \n   0.022754677   -0.098270674 \n\nfit %&gt;% vcov()\n\n                   Intercept blood.pressure      sex=male           age\nIntercept       3.0234267326  -2.096123e-03 -1.919335e-04 -1.799902e-03\nblood.pressure -0.0020961233   1.852472e-05 -4.542393e-06 -1.048266e-06\nsex=male       -0.0001919335  -4.542393e-06  1.691277e-02  3.642625e-05\nage            -0.0017999015  -1.048266e-06  3.642625e-05  4.340282e-05\ncholesterol    -0.0155094677  -5.297436e-07 -5.161492e-05 -1.592218e-06\ncholesterol'    0.0379635708   4.065325e-06  1.122060e-04  5.500943e-06\ncholesterol''  -0.1361901376  -1.938549e-05 -3.636389e-04 -2.046384e-05\n                 cholesterol  cholesterol' cholesterol''\nIntercept      -1.550947e-02  3.796357e-02 -1.361901e-01\nblood.pressure -5.297436e-07  4.065325e-06 -1.938549e-05\nsex=male       -5.161492e-05  1.122060e-04 -3.636389e-04\nage            -1.592218e-06  5.500943e-06 -2.046384e-05\ncholesterol     9.119299e-05 -2.315889e-04  8.424284e-04\ncholesterol'   -2.315889e-04  7.331333e-04 -2.936085e-03\ncholesterol''   8.424284e-04 -2.936085e-03  1.241586e-02\n\n## point estimate of beta_sex\nbeta_sex_hat &lt;- coef(fit)[\"sex=male\"]\n\n## standard error for coef of sex\nbeta_sex_se &lt;- sqrt(vcov(fit)[3,3])\n\n## we know beta_hat follows a normal distribution, with mean=beta_sex_hat, \n\nalpha &lt;- qnorm(0.975,mean=0,sd=1)\nalpha\n\n[1] 1.959964\n\nupper &lt;- exp(beta_sex_hat + alpha*beta_sex_se)\nlower &lt;- exp(beta_sex_hat - alpha*beta_sex_se)\n\nlower \n\nsex=male \n1.248739 \n\nupper \n\nsex=male \n2.079064 \n\n\nThe odds of the outcome for males is 1.61 times that for females (95% CI of 1.25 to 2.08) controlled for blood pressure, age and cholesterol.\n\n## log odds\nplot(Predict(fit, cholesterol))\n\n\n\n\n\n\n\n## probability\nplot(Predict(fit, cholesterol, fun = plogis), ylab = \"Probability\")\n\n\n\n\n\n\n\n## Odds\n\nplot(Predict(fit, cholesterol, fun = exp), ylab = \"Odds\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Tutorial 3: Logistic Regression 1</span>"
    ]
  },
  {
    "objectID": "Tutorial4/Tut4.html#recap-of-tutorial-3",
    "href": "Tutorial4/Tut4.html#recap-of-tutorial-3",
    "title": "4¬† Logistic Regression 2",
    "section": "",
    "text": "How would you calculate the estimated mean outcome (probability of otucome being positive, \\(E(Y) = P(Y=1)\\))\n\n\n\n\n\n\n\nCheck Answer\n\n\n\n\n\n\\(\\hat P(Y=1) = \\frac{exp(\\hat Z)}{1+exp(\\hat Z)}\\), where \\(Z\\) is the linear combination of predictors\n\n\n\n\nFor (generalized) linear regression \\(g(E(Y)) = \\beta_0 + \\beta_1 X_1 +\\ldots + \\beta_p X_p\\), the MLE estimators \\(\\boldsymbol{\\hat \\beta} = [ \\hat\\beta_0, \\hat \\beta_1,\\ldots \\hat \\beta_p]\\) follows a multivariate normal distribution, each \\(\\hat \\beta\\) is a normal distribution.\nif I have a R.V. \\(\\hat \\beta = [\\hat \\beta_1, \\hat \\beta_2]\\) follows a bivariate normal distribution, how do we calculate the variance for \\(\\beta_1+\\beta_2\\) , and for \\(a \\beta_1 + b\\beta_2\\)?\n\n\n\n\n\n\n\nCheck Answer\n\n\n\n\n\n\\[\n\\begin{bmatrix} \\beta_1 \\\\ \\beta_2 \\end{bmatrix} \\sim N\\left( \\begin{bmatrix} \\mu_1 \\\\ \\mu_2 \\end{bmatrix}, \\begin{bmatrix} \\sigma_1^2 & \\rho \\sigma_1 \\sigma_2 \\\\ \\rho \\sigma_1 \\sigma_2 & \\sigma_2^2 \\end{bmatrix} \\right),\n\\]\nwhere \\(\\rho \\sigma_1 \\sigma_2\\) is the covariance between \\(\\beta_1\\) and \\(\\beta_2\\).\nthe variance of \\(\\beta_1 + \\beta_2\\) is given by: \\[\n\\text{Var}(\\beta_1 + \\beta_2) = \\sigma_1^2 + \\sigma_2^2 + 2\\rho \\sigma_1 \\sigma_2.\n\\] \\[\n\\text{Var}(a\\beta_1 + b\\beta_2) = a^2\\sigma_1^2 + b^2\\sigma_2^2 + 2ab\\rho \\sigma_1 \\sigma_2.\n\\]\n\n\n\n\nGiven 1-3, do you know how can we calculate the variance of the estiamted probability of the outcome being positive (\\(\\hat P(Y=1)\\))?",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Tutorial 4: Logistic Regression 2</span>"
    ]
  },
  {
    "objectID": "Tutorial4/Tut4.html#tutorial-4",
    "href": "Tutorial4/Tut4.html#tutorial-4",
    "title": "4¬† Logistic Regression 2",
    "section": "4.2 Tutorial 4",
    "text": "4.2 Tutorial 4\n\nfit1 &lt;- lrm(y ~ blood.pressure + sex + age + rcs(cholesterol,4), data = tutdata)\n\n\nfit2 &lt;- lrm(y ~ blood.pressure + sex * (age + rcs(cholesterol,4)), data = tutdata)\nanova(fit2)\n\n                Wald Statistics          Response: y \n\n Factor                                           Chi-Square d.f. P     \n blood.pressure                                    0.26       1   0.6105\n sex  (Factor+Higher Order Factors)               38.71       5   &lt;.0001\n  All Interactions                                26.13       4   &lt;.0001\n age  (Factor+Higher Order Factors)               30.42       2   &lt;.0001\n  All Interactions                                 3.86       1   0.0495\n cholesterol  (Factor+Higher Order Factors)       23.78       6   0.0006\n  All Interactions                                22.47       3   0.0001\n  Nonlinear (Factor+Higher Order Factors)          5.33       4   0.2550\n sex * age  (Factor+Higher Order Factors)          3.86       1   0.0495\n sex * cholesterol  (Factor+Higher Order Factors) 22.47       3   0.0001\n  Nonlinear                                        4.79       2   0.0911\n  Nonlinear Interaction : f(A,B) vs. AB            4.79       2   0.0911\n TOTAL NONLINEAR                                   5.33       4   0.2550\n TOTAL INTERACTION                                26.13       4   &lt;.0001\n TOTAL NONLINEAR + INTERACTION                    26.81       6   0.0002\n TOTAL                                            62.26      10   &lt;.0001\n\nanova(fit2)\n\n                Wald Statistics          Response: y \n\n Factor                                           Chi-Square d.f. P     \n blood.pressure                                    0.26       1   0.6105\n sex  (Factor+Higher Order Factors)               38.71       5   &lt;.0001\n  All Interactions                                26.13       4   &lt;.0001\n age  (Factor+Higher Order Factors)               30.42       2   &lt;.0001\n  All Interactions                                 3.86       1   0.0495\n cholesterol  (Factor+Higher Order Factors)       23.78       6   0.0006\n  All Interactions                                22.47       3   0.0001\n  Nonlinear (Factor+Higher Order Factors)          5.33       4   0.2550\n sex * age  (Factor+Higher Order Factors)          3.86       1   0.0495\n sex * cholesterol  (Factor+Higher Order Factors) 22.47       3   0.0001\n  Nonlinear                                        4.79       2   0.0911\n  Nonlinear Interaction : f(A,B) vs. AB            4.79       2   0.0911\n TOTAL NONLINEAR                                   5.33       4   0.2550\n TOTAL INTERACTION                                26.13       4   &lt;.0001\n TOTAL NONLINEAR + INTERACTION                    26.81       6   0.0002\n TOTAL                                            62.26      10   &lt;.0001\n\n\ncompare two models using LRT. To use LRT, one model needs to be nested within a larger model.\n\nlrtest(fit1,fit2)\n\n\nModel 1: y ~ blood.pressure + sex + age + rcs(cholesterol, 4)\nModel 2: y ~ blood.pressure + sex * (age + rcs(cholesterol, 4))\n\n  L.R. Chisq         d.f.            P \n2.831616e+01 4.000000e+00 1.076137e-05 \n\n\nvisualize the effect modifications:\n\nplot(Predict(fit2,age,sex))\n\n\n\n\n\n\n\nplot(Predict(fit2,cholesterol,sex))\n\n\n\n\n\n\n\n\nwhat would the plots look like without effect modification:\n\nplot(Predict(fit1, age, sex = \"male\"))\n\n\n\n\n\n\n\nplot(Predict(fit1, age, sex = \"female\"))",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Tutorial 4: Logistic Regression 2</span>"
    ]
  },
  {
    "objectID": "Tutorial5/Tut5.html#tutorial-5",
    "href": "Tutorial5/Tut5.html#tutorial-5",
    "title": "5¬† Logistic Regression 3",
    "section": "5.2 Tutorial 5",
    "text": "5.2 Tutorial 5\n\nset.seed(1017)\nvalidate(fit2, B = 100)\n\n          index.orig training   test optimism index.corrected   n\nDxy           0.2832   0.3058 0.2682   0.0376          0.2456 100\nR2            0.0896   0.1059 0.0795   0.0263          0.0632 100\nIntercept     0.0000   0.0000 0.0389  -0.0389          0.0389 100\nSlope         1.0000   1.0000 0.8601   0.1399          0.8601 100\nEmax          0.0000   0.0000 0.0401   0.0401          0.0401 100\nD             0.0684   0.0817 0.0604   0.0213          0.0470 100\nU            -0.0020  -0.0020 0.0019  -0.0039          0.0019 100\nQ             0.0704   0.0837 0.0585   0.0252          0.0452 100\nB             0.2316   0.2288 0.2342  -0.0054          0.2369 100\ng             0.6230   0.6828 0.5790   0.1038          0.5192 100\ngp            0.1457   0.1566 0.1357   0.0209          0.1248 100\n\n\n\nindex.orig: The performance metric computed on the full dataset.\n\ntraining: The metric computed on the training dataset (bootstrap sample). - test: The metric computed on the test dataset (out-of-bootstrap sample).\n\noptimism: The difference between training and test performance, estimating overfitting.\n\nindex.corrected: The optimism-adjusted estimate of model performance. n: Number of bootstrap iterations (100)\n\ng (Gini coefficient): Related to AUC (Area Under the Curve)\nEmax (Maximum Calibration Error): Largest difference between predicted and observed probabilities.\n\n\ndff &lt;- resid(fit2, \"dffits\")\nplot(dff)\n\n\n\n\n\n\n\nshow.influence(which.influence(fit2), data.frame(tutdata, dffits = dff), report = c(\"dffits\"))\n\n    Count     sex cholesterol     dffits\n143     2  female   *274.0808  1.6108318\n173     4 *female   *138.5056 -1.1265158\n181     2 *male      146.4978  0.8026558\n408     2 *male      148.1649  0.6813596\n411     1  female    146.3522  0.5594519\n834     2  female   *146.6413 -0.4908278\n840     2  female   *147.0025 -0.5604495\n\n\nThe dffits statistic measures the influence of each observation on the fitted model. It represents the change in the predicted value for a data point if that data point were removed from the model.\n\n# Partial residuals are the residuals from the model, adjusted for the effect of the predictors that are not of primary interest\nresid(fit2, \"partial\", pl = \"loess\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNon-linearity: If the loess curve shows a clear non-linear relationship between a predictor and the residuals, you might need to consider transforming that predictor (e.g., using log or polynomial terms) to improve the model fit.\n\nOutliers or influential points: Any points that deviate significantly from the smoothed curve might be influential or outliers, warranting further investigation.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Tutorial 5: Logistic Regression 3</span>"
    ]
  },
  {
    "objectID": "Tutorial6/Tut6.html#recap-of-tutorial-5",
    "href": "Tutorial6/Tut6.html#recap-of-tutorial-5",
    "title": "6¬† Survival Analysis 1",
    "section": "",
    "text": "6.1.1 Method 1\nChange the reference level:\n\nsim_dat$x &lt;- relevel(sim_dat$x, ref = \"2\")\n\noutcome_m2 &lt;- glm(true_Y_bin ~ x, family = binomial(\"logit\"), data = sim_dat)\n\nsummary(outcome_m2)\n\n\nCall:\nglm(formula = true_Y_bin ~ x, family = binomial(\"logit\"), data = sim_dat)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   1.7280     0.2088   8.274  &lt; 2e-16 ***\nx1           -1.0489     0.2541  -4.128 3.65e-05 ***\nx3            0.7269     0.2574   2.824  0.00474 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 834.94  on 999  degrees of freedom\nResidual deviance: 760.88  on 997  degrees of freedom\nAIC: 766.88\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n\n6.1.2 Method 2\nAsk yourself the following questions:\n\nwhat is the estimand/goal?\n\n\\[\\beta_3 - \\beta_2\\]\n\nwhat is the estimator?\n\n\\[\\hat\\beta_3 - \\hat \\beta_2\\]\n\nwhat is the distribution of MLE estimator?\n\n\\(\\hat\\beta_3\\) and \\(\\hat \\beta_2\\) follow normal distribution. (one of the nice properties of MLE estimators)\nthen we can easily calculate the variance of \\[\\hat\\beta_3 - \\hat \\beta_2\\] using some math.\n\ncov_matrix &lt;- vcov(outcome_m)\n\nvar2 &lt;- cov_matrix[2,2]\nvar3 &lt;- cov_matrix[3,3]\ncov_b23 &lt;- cov_matrix[2,3]\n## refer to tutorial 4 for the calculation of var(A+B)\nvar_b3_minus_b2 &lt;- var2 + var3  - 2*cov_b23\n\nvar_b3_minus_b2 %&gt;% sqrt()\n\n[1] 0.2573678\n\nse &lt;- var_b3_minus_b2 %&gt;% sqrt()\n\nse\n\n[1] 0.2573678\n\n\nthe point estiamte is calculated as:\n\npoint_est &lt;- coef(outcome_m)[3]-coef(outcome_m)[2]\n\nso we conclude that \\(\\hat\\beta_3 - \\hat \\beta_2\\) follow a normal distribution with mean 0.7269048 and se 0.2573678\nFinaly question, what is the relationship between \\(\\hat\\beta_3 - \\hat \\beta_2\\) and OR??\n\\[log(OR) = \\hat\\beta_3 - \\hat \\beta_2\\] You can calculate the point estimate and confidence interval for the OR as well! Hooray!",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Survival Analysis 1</span>"
    ]
  },
  {
    "objectID": "Tutorial6/Tut6.html#tutorial-6",
    "href": "Tutorial6/Tut6.html#tutorial-6",
    "title": "6¬† Survival Analysis 1",
    "section": "6.2 Tutorial 6",
    "text": "6.2 Tutorial 6\nIn this and the following few tutorials, we will focus on survival analysis.\n\nlibrary(rms)\n\nLoading required package: Hmisc\n\n\n\nAttaching package: 'Hmisc'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    src, summarize\n\n\nThe following objects are masked from 'package:base':\n\n    format.pval, units\n\nlibrary(survival)\n\nWarning: package 'survival' was built under R version 4.3.2\n\nstr(mgus2)\n\n'data.frame':   1384 obs. of  11 variables:\n $ id    : num  1 2 3 4 5 6 7 8 9 10 ...\n $ age   : num  88 78 94 68 90 90 89 87 86 79 ...\n  ..- attr(*, \"label\")= chr \"AGE AT mgus_sp\"\n $ sex   : Factor w/ 2 levels \"F\",\"M\": 1 1 2 2 1 2 1 1 1 1 ...\n  ..- attr(*, \"label\")= chr \"Sex\"\n  ..- attr(*, \"format\")= chr \"$desex\"\n $ dxyr  : num  1981 1968 1980 1977 1973 ...\n $ hgb   : num  13.1 11.5 10.5 15.2 10.7 12.9 10.5 12.3 14.5 9.4 ...\n $ creat : num  1.3 1.2 1.5 1.2 0.8 1 0.9 1.2 0.9 1.1 ...\n $ mspike: num  0.5 2 2.6 1.2 1 0.5 1.3 1.6 2.4 2.3 ...\n  ..- attr(*, \"label\")= chr \"SEP SPIKE AT mgus_sp\"\n $ ptime : num  30 25 46 92 8 4 151 2 57 136 ...\n $ pstat : num  0 0 0 0 0 0 0 0 0 0 ...\n $ futime: num  30 25 46 92 8 4 151 2 57 136 ...\n $ death : num  1 1 1 1 1 1 1 1 0 1 ...\n\n\n\nfit1 &lt;- npsurv(Surv(futime,death)~sex,data=mgus2)\nlibrary(dplyr)\ntmp1 &lt;- summary(fit1) %&gt;% head(5) %&gt;% unlist()\ntmp &lt;- summary(fit1)\n\nkm_dat &lt;- data.frame(time = tmp$time,\n                       risk = tmp$n.risk,\n                     death = tmp$n.event) %&gt;%\n  mutate(surv_i = 1- death/risk)\n\n\n\nsurvplot(fit1,n.risk=TRUE)\n\n\n\n\n\n\n\n\n\nprint(fit1.mh &lt;- survdiff(Surv(futime, death) ~ sex, data = mgus2))\n\nCall:\nsurvdiff(formula = Surv(futime, death) ~ sex, data = mgus2)\n\n        N Observed Expected (O-E)^2/E (O-E)^2/V\nsex=F 631      423      471      4.88      9.67\nsex=M 753      540      492      4.67      9.67\n\n Chisq= 9.7  on 1 degrees of freedom, p= 0.002 \n\n\n\nsummary(fit1,times=12) # time is in months\n\nCall: npsurv(formula = Surv(futime, death) ~ sex, data = mgus2)\n\n                sex=F \n        time       n.risk      n.event     survival      std.err lower 95% CI \n     12.0000     569.0000      61.0000       0.9032       0.0118       0.8804 \nupper 95% CI \n      0.9266 \n\n                sex=M \n        time       n.risk      n.event     survival      std.err lower 95% CI \n      12.000      646.000      112.000        0.851        0.013        0.826 \nupper 95% CI \n       0.877 \n\n\n\n## risk difference at year 1\nRD &lt;- 0.9032- 0.851\nRD + 1.96*sqrt(0.0118^2 +0.013^2)\n\n[1] 0.08661126\n\nRD - 1.96*sqrt(0.0118^2 +0.013^2)\n\n[1] 0.01778874\n\n\nwe know that logHR follows a normal distribution. So the confidence interval for HR could be calculated as:\n\\[exp(logHR ¬± 1.96 \\cdot se_{logHR}) = exp(logHR)\\cdot exp(¬± 1.96 \\cdot se_{logHR}) = HR \\cdot exp(¬± 1.96 \\cdot se_{logHR}) \\]\n\n## HR\n## HR &lt;- (obs1/exp1)/(obs2/exp2)\nHR &lt;- (423/471)/(540/492)\n\nHR\n\n[1] 0.818259\n\n## variance_logHR &lt;- (1/exp1) + (1/exp2)\nse_logHR &lt;- sqrt( (1/471) +(1/492) )\n\n\n## lower bound\nHR *exp(-1.96*se_logHR)\n\n[1] 0.7211367\n\n## 95% CI\n## upper bound\nHR *exp(1.96*se_logHR)\n\n[1] 0.9284618",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Survival Analysis 1</span>"
    ]
  },
  {
    "objectID": "Tutorial8/Tut8.html",
    "href": "Tutorial8/Tut8.html",
    "title": "8¬† Survival Analysis 2",
    "section": "",
    "text": "8.1 Recap of Tutorial 7\nWe went over how KM estimator (for survival probability) is constructed for discrete time.\nWe will discuss continuous time in this tutorial.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Survival Analysis 2</span>"
    ]
  },
  {
    "objectID": "Tutorial8/Tut8.html#tutorial-8",
    "href": "Tutorial8/Tut8.html#tutorial-8",
    "title": "8¬† Survival Analysis 2",
    "section": "8.2 Tutorial 8",
    "text": "8.2 Tutorial 8\nToday we will cover diagnostics.\nPH assumption checked using Schoenfeld residual.\n\nlibrary(rms)\n\nLoading required package: Hmisc\n\n\n\nAttaching package: 'Hmisc'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    src, summarize\n\n\nThe following objects are masked from 'package:base':\n\n    format.pval, units\n\nlibrary(survival)\n\nWarning: package 'survival' was built under R version 4.3.2\n\nlibrary(lattice)\nstr(mgus2)\n\n'data.frame':   1384 obs. of  11 variables:\n $ id    : num  1 2 3 4 5 6 7 8 9 10 ...\n $ age   : num  88 78 94 68 90 90 89 87 86 79 ...\n  ..- attr(*, \"label\")= chr \"AGE AT mgus_sp\"\n $ sex   : Factor w/ 2 levels \"F\",\"M\": 1 1 2 2 1 2 1 1 1 1 ...\n  ..- attr(*, \"label\")= chr \"Sex\"\n  ..- attr(*, \"format\")= chr \"$desex\"\n $ dxyr  : num  1981 1968 1980 1977 1973 ...\n $ hgb   : num  13.1 11.5 10.5 15.2 10.7 12.9 10.5 12.3 14.5 9.4 ...\n $ creat : num  1.3 1.2 1.5 1.2 0.8 1 0.9 1.2 0.9 1.1 ...\n $ mspike: num  0.5 2 2.6 1.2 1 0.5 1.3 1.6 2.4 2.3 ...\n  ..- attr(*, \"label\")= chr \"SEP SPIKE AT mgus_sp\"\n $ ptime : num  30 25 46 92 8 4 151 2 57 136 ...\n $ pstat : num  0 0 0 0 0 0 0 0 0 0 ...\n $ futime: num  30 25 46 92 8 4 151 2 57 136 ...\n $ death : num  1 1 1 1 1 1 1 1 0 1 ...\n\ndd &lt;- datadist(mgus2)\noptions(datadist=\"dd\")\nstr(mgus2)\n\n'data.frame':   1384 obs. of  11 variables:\n $ id    : num  1 2 3 4 5 6 7 8 9 10 ...\n $ age   : num  88 78 94 68 90 90 89 87 86 79 ...\n  ..- attr(*, \"label\")= chr \"AGE AT mgus_sp\"\n $ sex   : Factor w/ 2 levels \"F\",\"M\": 1 1 2 2 1 2 1 1 1 1 ...\n  ..- attr(*, \"label\")= chr \"Sex\"\n  ..- attr(*, \"format\")= chr \"$desex\"\n $ dxyr  : num  1981 1968 1980 1977 1973 ...\n $ hgb   : num  13.1 11.5 10.5 15.2 10.7 12.9 10.5 12.3 14.5 9.4 ...\n $ creat : num  1.3 1.2 1.5 1.2 0.8 1 0.9 1.2 0.9 1.1 ...\n $ mspike: num  0.5 2 2.6 1.2 1 0.5 1.3 1.6 2.4 2.3 ...\n  ..- attr(*, \"label\")= chr \"SEP SPIKE AT mgus_sp\"\n $ ptime : num  30 25 46 92 8 4 151 2 57 136 ...\n $ pstat : num  0 0 0 0 0 0 0 0 0 0 ...\n $ futime: num  30 25 46 92 8 4 151 2 57 136 ...\n $ death : num  1 1 1 1 1 1 1 1 0 1 ...\n\nfit1 &lt;- cph(Surv(futime, death) ~ sex, data = mgus2, x = TRUE, y = TRUE)\nprint(fit1.zph &lt;- cox.zph(fit1))\n\n       chisq df    p\nsex     2.47  1 0.12\nGLOBAL  2.47  1 0.12\n\nplot(fit1.zph)\n\n\n\n\n\n\n\n\ncheck function form for continuous covariates using martingale residual\n\nres &lt;- resid(fit1)\nres.lo &lt;- loess(res~hgb,data=mgus2)\nres.ols &lt;- ols(res~rcs(hgb,3),data=mgus2)\nres.ols &lt;- ols(res~rcs(hgb,5),data=mgus2)\nplot(Predict(res.ols,hgb),addpanel=function(...){\n panel.points(mgus2$hgb,res)\n panel.lines(seq(5,20,length.out=25),\n predict(res.lo,seq(5,20,length.out=25)),col=\"red\")},\n ylim=1.15*range(res),\n ylab=\"Martingale Residual\",xlab=\"Hgb\")\n\n\n\n\n\n\n\n\n\nfit2 &lt;- cph(Surv(futime, death) ~ sex * (rcs(age, 5) + rcs(hgb, 5) + rcs(creat,\n    5)), data = mgus2, x = TRUE, y = TRUE)\nanova(fit2)\n\n                Wald Statistics          Response: Surv(futime, death) \n\n Factor                                     Chi-Square d.f. P     \n sex  (Factor+Higher Order Factors)          40.49     13   0.0001\n  All Interactions                           14.79     12   0.2529\n age  (Factor+Higher Order Factors)         251.27      8   &lt;.0001\n  All Interactions                            0.27      4   0.9919\n  Nonlinear (Factor+Higher Order Factors)     7.46      6   0.2805\n hgb  (Factor+Higher Order Factors)          56.74      8   &lt;.0001\n  All Interactions                            0.36      4   0.9859\n  Nonlinear (Factor+Higher Order Factors)    12.45      6   0.0527\n creat  (Factor+Higher Order Factors)        44.76      8   &lt;.0001\n  All Interactions                           13.57      4   0.0088\n  Nonlinear (Factor+Higher Order Factors)    28.66      6   0.0001\n sex * age  (Factor+Higher Order Factors)     0.27      4   0.9919\n  Nonlinear                                   0.23      3   0.9718\n  Nonlinear Interaction : f(A,B) vs. AB       0.23      3   0.9718\n sex * hgb  (Factor+Higher Order Factors)     0.36      4   0.9859\n  Nonlinear                                   0.08      3   0.9945\n  Nonlinear Interaction : f(A,B) vs. AB       0.08      3   0.9945\n sex * creat  (Factor+Higher Order Factors)  13.57      4   0.0088\n  Nonlinear                                  11.36      3   0.0100\n  Nonlinear Interaction : f(A,B) vs. AB      11.36      3   0.0100\n TOTAL NONLINEAR                             48.33     18   0.0001\n TOTAL INTERACTION                           14.79     12   0.2529\n TOTAL NONLINEAR + INTERACTION               56.21     21   &lt;.0001\n TOTAL                                      468.25     25   &lt;.0001\n\n\n\nset.seed(1017)\nvalidate(fit2, B = 100)\n\nWarning in fitter(..., strata = strata, rownames = rownames, offset = offset, :\nLoglik converged before variable 20 ; coefficient may be infinite.\nWarning in fitter(..., strata = strata, rownames = rownames, offset = offset, :\nLoglik converged before variable 20 ; coefficient may be infinite.\n\n\n      index.orig training   test optimism index.corrected   n\nDxy       0.4023   0.4091 0.3952   0.0138          0.3885 100\nR2        0.3091   0.3183 0.2874   0.0309          0.2782 100\nSlope     1.0000   1.0000 0.8971   0.1029          0.8971 100\nD         0.0407   0.0423 0.0373   0.0050          0.0357 100\nU        -0.0002  -0.0002 0.0033  -0.0035          0.0033 100\nQ         0.0408   0.0424 0.0340   0.0084          0.0324 100\ng         0.9390   0.9605 0.8614   0.0991          0.8399 100",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Survival Analysis 2</span>"
    ]
  },
  {
    "objectID": "Tutorial7/Tut7.html#tutorial-7",
    "href": "Tutorial7/Tut7.html#tutorial-7",
    "title": "7¬† Survival Analysis 2",
    "section": "7.2 Tutorial 7",
    "text": "7.2 Tutorial 7\nWe will cover parametric method, specifically, Cox regression, and time is treated as continuous.\n\nlibrary(rms)\n\nLoading required package: Hmisc\n\n\n\nAttaching package: 'Hmisc'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    src, summarize\n\n\nThe following objects are masked from 'package:base':\n\n    format.pval, units\n\nlibrary(survival)\n\nWarning: package 'survival' was built under R version 4.3.2\n\ndd &lt;- datadist(mgus2)\noptions(datadist=\"dd\")\n\nfit.km &lt;- npsurv(Surv(futime,death)~sex,data=mgus2)\n\n### Check exponential\nplot(fit.km,fun=\"cumhaz\")\n\n\n\n\n\n\n\n## check proportionality (difference)\nplot(fit.km,fun=\"cloglog\")\n\n\n\n\n\n\n\n## parametric model exp\nprint(fit1 &lt;- psm(Surv(futime,death)~sex,\n                  data=mgus2,dist=\"exponential\"))\n\nParametric Survival Model: Exponential Distribution\n\npsm(formula = Surv(futime, death) ~ sex, data = mgus2, dist = \"exponential\")\n\n                 Model Likelihood     Discrimination    \n                       Ratio Test            Indexes    \nObs     1384    LR chi2      9.99     R2       0.007    \nEvents   963    d.f.            1    R2(1,1384)0.006    \nsigma 1.0000    Pr(&gt; chi2) 0.0016     R2(1,963)0.009    \n                                      Dxy      0.062    \n\n            Coef    S.E.   Wald Z Pr(&gt;|Z|)\n(Intercept)  5.0344 0.0486 103.54 &lt;0.0001 \nsex=M       -0.2045 0.0649  -3.15 0.0016  \n\n### HR for sex: M vs. F\nexp(-coef(fit1)[2])\n\n   sex=M \n1.226934 \n\n### Survival time ratio\nsummary(fit1,sex=\"F\")\n\n             Effects              Response : Surv(futime, death) \n\n Factor               Low High Diff. Effect   S.E.    Lower 0.95 Upper 0.95\n sex - M:F            1   2    NA    -0.20452 0.06493 -0.33189   -0.077146 \n  Survival Time Ratio 1   2    NA     0.81504      NA  0.71757    0.925750 \n\nsurvplot(fit1,sex)\n## KM non-parametric\nsurvplot(fit.km,add=TRUE,label.curves=FALSE,conf=\"none\",col=\"red\")\n\n\n\n\n\n\n\n\n\n## fit weibull\nprint(fit2 &lt;- psm(Surv(futime,death)~sex,data=mgus2,dist=\"weibull\"))\n\nParametric Survival Model: Weibull Distribution\n\npsm(formula = Surv(futime, death) ~ sex, data = mgus2, dist = \"weibull\")\n\n                 Model Likelihood     Discrimination    \n                       Ratio Test            Indexes    \nObs     1384    LR chi2      9.18     R2       0.007    \nEvents   963    d.f.            1    R2(1,1384)0.006    \nsigma 1.1016    Pr(&gt; chi2) 0.0024     R2(1,963)0.008    \n                                      Dxy      0.062    \n\n            Coef    S.E.   Wald Z Pr(&gt;|Z|)\n(Intercept)  5.0494 0.0538 93.87  &lt;0.0001 \nsex=M       -0.2162 0.0716 -3.02  0.0025  \nLog(scale)   0.0967 0.0278  3.48  0.0005  \n\n# phreg\n# survreg\n\n### HR for sex: M vs. F\n## weibull model: AFT parametrization\n## -gamma /scale\nexp(-coef(fit2)[2]/fit2$scale)\n\n   sex=M \n1.216865 \n\n### Survival time ratio\n\nsummary(fit2,sex=\"F\")\n\n             Effects              Response : Surv(futime, death) \n\n Factor               Low High Diff. Effect   S.E.     Lower 0.95 Upper 0.95\n sex - M:F            1   2    NA    -0.21622 0.071609 -0.35669   -0.075742 \n  Survival Time Ratio 1   2    NA     0.80556       NA  0.69999    0.927060 \n\nsurvplot(fit2,sex)\nsurvplot(fit.km,add=TRUE,label.curves=FALSE,conf=\"none\",col=\"red\")\n\n\n\n\n\n\n\n## ---------------------------------------------------------------------------------------------\nsurvplot(fit.km,conf=\"none\")\nsurvplot(fit1,sex,add=TRUE,label.curves=FALSE,col=\"red\")\nsurvplot(fit2,sex,add=TRUE,label.curves=FALSE,col=\"blue\")\n\n\n\n\n\n\n\n\nCox PH model\n\nprint(fit1.cox &lt;- cph(Surv(futime,death)~sex,data=mgus2))\n\nCox Proportional Hazards Model\n\ncph(formula = Surv(futime, death) ~ sex, data = mgus2)\n\n                       Model Tests     Discrimination    \n                                              Indexes    \nObs      1384    LR chi2      9.68     R2       0.007    \nEvents    963    d.f.            1    R2(1,1384)0.006    \nCenter 0.1098    Pr(&gt; chi2) 0.0019     R2(1,963)0.009    \n                 Score chi2   9.65     Dxy      0.062    \n                 Pr(&gt; chi2) 0.0019                       \n\n      Coef   S.E.   Wald Z Pr(&gt;|Z|)\nsex=M 0.2017 0.0651 3.10   0.0019  \n\nsummary(fit1.cox,sex=\"F\")\n\n             Effects              Response : Surv(futime, death) \n\n Factor        Low High Diff. Effect  S.E.     Lower 0.95 Upper 0.95\n sex - M:F     1   2    NA    0.20174 0.065062 0.074221   0.32926   \n  Hazard Ratio 1   2    NA    1.22350       NA 1.077000   1.38990   \n\nprint(fit2.cox &lt;- cph(Surv(futime,death)~sex+rcs(age,4),data=mgus2))\n\nCox Proportional Hazards Model\n\ncph(formula = Surv(futime, death) ~ sex + rcs(age, 4), data = mgus2)\n\n                      Model Tests     Discrimination    \n                                             Indexes    \nObs     1384    LR chi2    396.10     R2       0.249    \nEvents   963    d.f.            4    R2(4,1384)0.247    \nCenter 3.248    Pr(&gt; chi2) 0.0000     R2(4,963)0.334    \n                Score chi2 408.99     Dxy      0.334    \n                Pr(&gt; chi2) 0.0000                       \n\n      Coef    S.E.   Wald Z Pr(&gt;|Z|)\nsex=M  0.3605 0.0657  5.48  &lt;0.0001 \nage    0.0380 0.0108  3.52  0.0004  \nage'   0.0473 0.0214  2.21  0.0272  \nage'' -0.2396 0.1233 -1.94  0.0520  \n\n### LRT\n\nlrtest(fit1.cox,fit2.cox)\n\n\nModel 1: Surv(futime, death) ~ sex\nModel 2: Surv(futime, death) ~ sex + rcs(age, 4)\n\nL.R. Chisq       d.f.          P \n    386.42       3.00       0.00 \n\n### Adjusted effect for sex\n\nsummary(fit2.cox,sex=\"F\")\n\n             Effects              Response : Surv(futime, death) \n\n Factor        Low High Diff. Effect  S.E.     Lower 0.95 Upper 0.95\n age           63  79   16    1.13420 0.094988 0.94802    1.32040   \n  Hazard Ratio 63  79   16    3.10870       NA 2.58060    3.74480   \n sex - M:F      1   2   NA    0.36049 0.065742 0.23164    0.48935   \n  Hazard Ratio  1   2   NA    1.43400       NA 1.26070    1.63120   \n\n### Plot of age effect\n# dev.off()\nplot(Predict(fit2.cox,age,sex))",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Survival Analysis 2</span>"
    ]
  }
]